---
title: 重温《Building Effective Agents》
date: 2026-01-18
summary: 轻流程，重智能，强观测——我对 Agent 开发的一点思考
tags:
  - AI
  - Agent
  - LLM
---

最近又翻了一遍 Anthropic 的 [Building Effective Agents](https://www.anthropic.com/engineering/building-effective-agents)。这篇文章是 2024 年底发布的，由 Erik Schluntz 和 Barry Zhang 撰写，是他们和几十个团队合作做 Agent 之后的经验总结。时隔一段时间再读，有了一些新的体会。

## 原文概览

文章首先定义了 **Agentic Systems（智能体系统）** 这个大概念，然后把它分成两类：

### Workflow vs Agent

- **Workflow**：多个 LLM 按照**预定义的流程**协作，人来设计路径
- **Agent**：LLM **自主决定**下一步做什么、用什么工具、何时停止

两者的核心区别在于**自主程度**。Workflow 跑在预设的轨道上，Agent 则拥有更高的自由度。

### 五种 Workflow 模式

文章详细介绍了五种构建 Workflow 的模式：

| 模式 | 说明 | 典型场景 |
|------|------|----------|
| **Prompt Chaining** | 将任务拆成多个步骤，前一步的输出作为后一步的输入 | 先生成文档，再翻译成另一种语言 |
| **Routing** | 第一个 LLM 调用决定后续走哪条分支 | 根据用户问题类型分发给不同的专业模型 |
| **Parallelization** | 任务拆分后并行处理，或通过投票机制聚合结果 | 多模型同时评估，取多数意见 |
| **Orchestrator-Workers** | 编排器触发多个 Worker，最后综合结果 | 复杂任务分解给多个子任务处理器 |
| **Evaluator-Optimizer** | 一个模型生成，另一个模型评估，循环优化 | 代码生成后自动测试并修复 |

### Agent 模式

相比 Workflow 的预定义路径，Agent 模式让 LLM 自己决定执行流程。它的基础构建块是 **Augmented LLM**——一个增强了检索、工具调用、记忆能力的语言模型。

### 核心建议

文章反复强调几个原则：

1. **简单优先**：能用简单方案就不要上复杂系统
2. **谨慎使用框架**：框架容易增加不必要的抽象层，建议先用原生 API
3. **工具质量决定上限**：在 SWE-bench 上，他们花在优化工具的时间比优化 prompt 还多

> Success isn't about building the most sophisticated system. It's about building the right system for your needs.

---

这些内容都很实用。但读完之后，我脑子里一直在转一个问题：**我们真的需要那么多"流程"吗？**

## 为什么我们总想设计"流程"？

大概是程序员的本能吧——遇到问题就想设计一套系统来解决它。看到 AI 会犯错，第一反应是"我来加个检查点"；看到任务复杂，第一反应是"我来拆成几个步骤"。

但这种思路有个隐含假设：**我们比 AI 更懂这个任务应该怎么做**。

在模型能力还比较弱的时候，这个假设是成立的。我们确实需要用代码来"兜住" AI，防止它跑偏。

但现在呢？模型越来越强，很多时候它对任务的理解已经不比我们差了。这时候如果还坚持用硬编码的流程去约束它，反而可能适得其反——就像给一个有经验的员工写一份事无巨细的操作手册，既没必要，又限制了他发挥。

## 轻流程，重工具

我现在越来越倾向于另一种思路：**少设计流程，多打磨工具**。

与其花时间想"第一步做什么、第二步做什么、遇到什么情况走哪个分支"，不如把精力放在：给 AI 提供一套好用的工具，然后让它自己决定怎么组合使用。

文章里有一句话我印象很深：

> 在 SWE-bench 上，我们花在优化工具的时间比优化 prompt 还多。

这说明什么？**工具的质量，直接决定了 Agent 的能力上限**。

一个设计良好的工具，AI 拿起来就能用对。一个设计糟糕的工具，你再怎么在 prompt 里强调"小心使用"，它还是会出错。

文章举了个例子：他们发现 Agent 在相对路径上老是搞错，后来改成强制使用绝对路径，问题就没了。这不是靠更复杂的流程解决的，而是靠更好的工具设计。

## MCP 和 Skill：工具的两种形态

说到工具，现在有两种主流的提供方式：

一种是 **MCP（Model Context Protocol）**，Anthropic 自己推的标准。它定义了一套协议，让不同的工具可以用统一的方式接入 AI。

另一种是 **Skill**，更像是一段预定义的能力，AI 可以在需要的时候调用。

形式不重要，重要的是：**把工具做得足够好，足够稳定，足够清晰**。接口要直观，行为要可预期，文档要完善。做到这些，剩下的交给 AI 自己来决定就好了。

## 一点实践感受

最近在用 Claude Code 写代码，感受挺明显的。

它有很多内置的工具——读文件、写文件、搜索、执行命令、调用 MCP 服务等等。作为用户，我不需要告诉它"先搜索相关代码，再读取文件，再分析，再修改"——我只要说清楚我想要什么，它自己就会决定该怎么做。

有时候它的做法和我预想的不一样，但结果往往也能达到目的。这就是让 AI 自主决策的好处：它可能会找到一些我没想到的路径。

当然，这建立在工具本身足够可靠的基础上。如果某个工具不稳定，或者行为不可预期，那 AI 用起来也会出问题——这时候就是工具的问题，不是流程的问题。

## 那人类怎么办？

写到这里我自己也在想：如果让 AI 自己决定怎么做，那我作为开发者（或者用户），怎么知道它在干什么？万一它跑偏了怎么办？

我目前的答案是：**看着它干**。

这听起来有点废话，但我觉得这是一个思路上的转变。以前我们习惯"事前约束"——提前告诉它只能这样做。现在可能更好的方式是"事中观测"——让它自己干，但我能看到它在做什么。

还是拿 Claude Code 举例。它每次调用工具、读取文件、执行命令，我都能实时看到。我不需要事先规定"你必须先搜索再读取再修改"，但如果它的方向不对，我可以随时喊停、随时纠正。

这种感觉有点像带新人：与其写一份事无巨细的操作手册让他照着做，不如让他自己想办法，但过程中保持沟通——"你打算怎么做？""做到哪了？""遇到什么问题？"

所以我现在觉得，**可观测性（Observability）** 可能是"重智能"思路下最重要的配套设施。工具调用日志、思考过程展示、关键操作确认……这些不是锦上添花，而是让人敢于放手的前提。

## 写在最后

回到最开始的问题：我们真的需要那么多"流程"吗？

我目前的想法是：**看情况，但可能没我们以为的那么多**。

金融、医疗这种场景，预定义流程肯定还是要的。但对于大多数开发场景，我越来越倾向于：流程做轻一点，工具做好一点，观测做全一点。然后相信模型能找到合理的路径。

毕竟，我们用 AI 就是因为它能处理复杂性。如果所有逻辑都硬编码了，AI 的价值在哪呢？

当然，这只是我目前的想法。模型能力还在快速迭代，最佳实践也在变。也许过一段时间再读这篇文章，又会有不一样的理解。

---

*原文：[Building Effective Agents](https://www.anthropic.com/engineering/building-effective-agents)*
